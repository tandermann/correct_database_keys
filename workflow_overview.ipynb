{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning database keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The challenge\n",
    "Data scientists often rely on data retrieved from public databases. One issue particularly with big databases can be that there is no or insufficient quality control of the entered data. Here we work on a solution for the issues of different taxon names (due to misspellings or different taxonomy) in fossil databases, but the scripts can be applied to any database that is supposed to contain categorical data, yet contains inconsistent strings for many of these categories.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fossil data in this project is retrieved from a set of online databases, namely the Paleobiology Database ([PBDB](https://paleobiodb.org/#/)), [NOW](http://www.helsinki.fi/science/now/) database, [Sahul](https://www.nature.com/articles/sdata201653) database and [Neotoma](https://www.neotomadb.org/) database. All databases were download in their entirety and the data was joined by extracting only the columns of interest, which are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────┬─────────┬─────────┬─────┬─────┬────────┬───────┬──────────┬───────────┐\n",
      "│ name │ max_age │ min_age │ lat │ lng │ family │ order │ database │ reference │\n",
      "└──────┴─────────┴─────────┴─────┴─────┴────────┴───────┴──────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import tabletext\n",
    "\n",
    "data = [['name','max_age','min_age','lat','lng','family','order','database','reference']]\n",
    "print(tabletext.to_text(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### The fossil data file looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\tmax_age\tmin_age\tlat\tlng\tfamily\torder\tdatabase\n",
      "Aaptorcytes ivyi\t57.7\t57.3\t44.678056\t-106.978889\tPalaeoryctidae\tCimolesta\tnow\n",
      "Aaptorcytes ivyi\t57.3\t56.3\t44.678056\t-106.978889\tPalaeoryctidae\tCimolesta\tnow\n",
      "Aaptoryctes ivyi\t61.7\t56.8\t44.693604\t-108.3554\tPalaeoryctidae\tCimolesta\tpbdb\n",
      "Aaptoryctes ivyi\t56.8\t55.8\t44.299999\t-109.0\tPalaeoryctidae\tCimolesta\tpbdb\n",
      "Aaptoryctes ivyi\t61.7\t56.8\t42.700001\t-110.099998\tPalaeoryctidae\tCimolesta\tpbdb\n",
      "Abderites meridionalis\t17.5\t16.3\t-48.516666\t-69.716667\tAbderitidae\tPaucituberculata\tpbdb\n",
      "Abderites meridionalis\t16.3\t15.5\t-44.577498999999996\t-71.220833\tAbderitidae\tPaucituberculata\tpbdb\n",
      "Abderites meridionalis\t23.03\t15.97\t-45.666668\t-68.666664\tAbderitidae\tPaucituberculata\tpbdb\n",
      "Abderites meridionalis\t17.5\t16.3\t-50.599998\t-69.083336\tAbderitidae\tPaucituberculata\tpbdb\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#skipping the reference column since it is very messy\n",
    "head -n 10 data/joined_databases.txt | cut -f 1-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The issue with the names in this database is that there are some misspellings, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adunator fredericki\t63.3\t61.7\t41.301398999999996\t-107.735703\t\tMacroscelidea\tpbdb\n",
      "Adunator fredricki\t61.3\t60.9\t41.14555\t-104.80194399999999\tLitocheridae\tEulipotyphla\tnow\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sed '1045q;d' data/joined_databases.txt | cut -f 1-8\n",
    "sed '1046q;d' data/joined_databases.txt | cut -f 1-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If we were to categorize the data, these would end up being two different taxa, even though they are both occurrences of the same species. We therefore need to join cases like this into the same category by clustering similar textstrings. However this clustering has to be done carfully since there are also cases where two different taxa names have very similar spellings and differ in only two letters as e.g.:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjidaumo maximus\t33.9\t33.3\t42.400002\t-103.800003\tEomyidae\tRodentia\tpbdb\n",
      "Adjidaumo minimus\t36.5\t35.7\t43.029722\t-106.80916699999999\tEomyidae\tRodentia\tnow\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sed '977q;d' data/joined_databases.txt | cut -f 1-8\n",
    "sed '978q;d' data/joined_databases.txt | cut -f 1-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### These names should not be clustered but kept separately, as they represent two legitimate different categories (species)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "First, we calculate the pairwise distance between any pair of strings in the taxon list and store these in a similarity matrix, using a python function, which is stored in the bin directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/1501 rows of data\n",
      "Processed 200/1501 rows of data\n",
      "Processed 300/1501 rows of data\n",
      "Processed 400/1501 rows of data\n",
      "Processed 500/1501 rows of data\n",
      "Processed 600/1501 rows of data\n",
      "Processed 700/1501 rows of data\n",
      "Processed 800/1501 rows of data\n",
      "Processed 900/1501 rows of data\n",
      "Processed 1000/1501 rows of data\n",
      "Processed 1100/1501 rows of data\n",
      "Processed 1200/1501 rows of data\n",
      "Processed 1300/1501 rows of data\n",
      "Processed 1400/1501 rows of data\n",
      "Processed 1500/1501 rows of data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append(\"./bin\")\n",
    "import create_simmatrix\n",
    "import pandas as pd\n",
    "from scipy.io import mmwrite\n",
    "\n",
    "fossil_file = pd.read_csv('/Users/tobias/GitHub/correct_database_keys/data/joined_databases.txt',sep='\\t',low_memory=False)\n",
    "\n",
    "#reduce the original dataset to the first 1500 lines, for faster computation\n",
    "sample_set_fossil_file = fossil_file.loc[:1500]\n",
    "taxon_names = list(sample_set_fossil_file.name)\n",
    "\n",
    "matrix = create_simmatrix.create_simmatrix(taxon_names)\n",
    "\n",
    "# write to Matrix Market format for passing to DBSCAN\n",
    "mmwrite('./intermediate_files/taxon_sim_names.mtx', matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
